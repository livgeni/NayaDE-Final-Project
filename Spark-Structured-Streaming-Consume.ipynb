{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from sys import stdout\n",
    "\n",
    "formatter = logging.Formatter(\n",
    "    '%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "console_handler = logging.StreamHandler(stdout)\n",
    "console_handler.setFormatter(formatter)\n",
    "\n",
    "logger = logging.getLogger('opensky.spark_consumer')\n",
    "logger.addHandler(console_handler)\n",
    "logger.setLevel('INFO')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrame\n",
    "import pandas as pd\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql import functions as F\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "# # Topics/Brokers\n",
    "topic_real_time_states = \"real-time-states\"\n",
    "topic_sparse_states = 'sparse_states'\n",
    "topic_enriched_real_time_states = 'enriched-real-rime-states'\n",
    "broker = \"localhost:9092\"\n",
    "\n",
    "host_name = 'cnt7-naya-cdh6'\n",
    "hive_host = \"localhost\"\n",
    "hdfs_host = \"localhost\"\n",
    "hdfs_port = 8020\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.1 pyspark-shell'\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"StructuredRealTimeState\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "schema = T.ArrayType(T.StructType()\\\n",
    "                     .add(\"time\", T.TimestampType())\\\n",
    "                     .add(\"icao24\", T.StringType())\\\n",
    "                     .add(\"callsign\", T.StringType())\\\n",
    "                     .add(\"last_contact\", T.TimestampType())\\\n",
    "                     .add(\"longitude\", T.FloatType())\\\n",
    "                     .add(\"latitude\", T.FloatType())\\\n",
    "                     .add(\"baro_altitude\", T.FloatType())\\\n",
    "                     .add(\"on_ground\", T.IntegerType())\\\n",
    "                     .add(\"velocity\", T.FloatType())\\\n",
    "                     .add(\"geo_altitude\", T.FloatType())\\\n",
    "                     .add(\"squawk\", T.StringType())\\\n",
    "                     .add(\"position_source\", T.IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import urllib\n",
    "from impala.dbapi import connect\n",
    "\n",
    "def drop_old_partitions(impala_conn: connect, table_name:str, table_src_path:str, \n",
    "                             partition_name:str, earliest_time_to_keep:datetime.timestamp):\n",
    "    \"\"\"\n",
    "    Drop old partitions at path <table_src_path>, drop all partitions older than <earliest_time_to_keep>\n",
    "    First drop partitions through impala client (impala.dbapi), then delete folders\n",
    "    \"\"\"\n",
    "    logger.debug(f'dropping old partitions for table {table_name}, all partitions oldert than {earliest_time_to_keep}')\n",
    "    \n",
    "    fs = pa.hdfs.connect(\n",
    "            host=hdfs_host, \n",
    "            port=hdfs_port, \n",
    "            user='hdfs', \n",
    "            kerb_ticket=None, \n",
    "            driver='libhdfs', \n",
    "            extra_conf=None)\n",
    "\n",
    "\n",
    "    partition_pattern = r'(.+\\=(.+))'\n",
    "    partitions_paths = fs.ls(table_src_path)\n",
    "    partitions_tup = [re.findall(partition_pattern, partition_path) for partition_path in partitions_paths]\n",
    "    # partitions_tup is of form : (partition path, partition date)\n",
    "    # urllib.parse.unquote in order to turn special chars as %3A to regular (:) , aslo flatten the list\n",
    "    partitions_tup = [(pt[0][0], urllib.parse.unquote(pt[0][1])) for pt in partitions_tup if len(pt) > 0]\n",
    "    partitions_dict = {date : path for path, date in partitions_tup}\n",
    "\n",
    "    partitions_to_delete = [p_d for p_d in partitions_dict.keys() if datetime.strptime(p_d, '%Y-%m-%d %H:%M:%S') < earliest_time_to_keep]\n",
    "\n",
    "    # here partitions to_delete holds all partitions that should be deleted\n",
    "\n",
    "    crsr = impala_conn.cursor()\n",
    "\n",
    "    try:\n",
    "        for part_key in partitions_to_delete:\n",
    "            crsr.execute(f'alter table {table_name} drop if exists partition ({partition_name}=\"{part_key}\");')\n",
    "            fs.delete(partitions_dict[part_key], recursive=True)\n",
    "            logger.debug(f'deleted : {partitions_dict[part_key]}')\n",
    "    except Exception as ex:\n",
    "        logger.Error(ex)\n",
    "    finally:\n",
    "        crsr.close()\n",
    "\n",
    "\n",
    "import pyarrow as pa\n",
    "from datetime import datetime, timedelta\n",
    "from impala.dbapi import connect\n",
    "from os import path\n",
    "\n",
    "\n",
    "def write_to_tables(df: DataFrame, epoch_id):\n",
    "    last_hour_table_name = 'last_hour'\n",
    "    target_database = 'opensky_network'\n",
    "    \n",
    "    root_data_path = '/user/naya/FinalProject/'\n",
    "    last_hour_path = path.join(root_data_path, 'last_hour')\n",
    "    last_day_path = path.join(root_data_path, 'last_day')\n",
    "    last_week_path = path.join(root_data_path,'last_week')\n",
    "    \n",
    "    logger.info(f'write_to_hive: epoch_id: {epoch_id} len: {df.count()}')\n",
    "    # write to each table - minutes, hours, days\n",
    "    if df.count() != 0:\n",
    "        df.persist()\n",
    "        logger.debug(f'trying to write to : {last_hour_path}')\n",
    "        df.withColumn('date_minute', F.date_trunc('minute', df.time))\\\n",
    "                    .write\\\n",
    "                    .mode(\"append\")\\\n",
    "                    .partitionBy('date_minute')\\\n",
    "                    .parquet(f'hdfs://localhost:8020/{last_hour_path}')\n",
    "        logger.debug(f'Trying to write to : {last_day_path}')\n",
    "        df.withColumn('date_hour', F.date_trunc('hour', df.time))\\\n",
    "                    .write\\\n",
    "                    .mode(\"append\")\\\n",
    "                    .partitionBy('date_hour')\\\n",
    "                    .parquet(f'hdfs://localhost:8020/{last_day_path}')\n",
    "        df.unpersist()\n",
    "\n",
    "        impala_conn = connect(host=host_name, database = target_database, user = 'naya', password = 'naya', auth_mechanism = 'NOSASL')\n",
    "\n",
    "        drop_old_partitions(impala_conn, 'states_last_hour', last_hour_path, \n",
    "                                 'date_minute', datetime.now() - timedelta(hours=1))\n",
    "        drop_old_partitions(impala_conn, 'states_last_day', last_day_path, \n",
    "                                 'date_hour', datetime.now() - timedelta(hours=24))\n",
    "\n",
    "        impala_crsr = impala_conn.cursor()\n",
    "        try:\n",
    "            for table_name in ['states_last_hour', 'states_last_day']:\n",
    "                impala_crsr.execute(f'alter table {table_name} recover partitions;')\n",
    "                impala_crsr.execute(f'refresh {table_name};')\n",
    "        except Exception as ex:\n",
    "            logger.error(ex)\n",
    "        finally:\n",
    "            impala_crsr.close()\n",
    "            \n",
    "#         crsr.execute(f'show partitions {table_name};')\n",
    "#         logger.debug([d for d, *rest in crsr.fetchall()])\n",
    "    \n",
    "#     impala_client.table(last_hour_table_name).drop_partition('date_minute=2019-12-28 12:13:00')\n",
    "#     impala_client.table(last_hour_table_name).alter()\n",
    "#     impala_client.table(last_hour_table_name).refresh()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from multiprocessing import  Pool\n",
    "from shapely.geometry.point import Point as ShapelyPoint\n",
    "from pyspark.sql.functions import udf\n",
    "import time\n",
    "\n",
    "# Add country to each row\n",
    "# Load countries file\n",
    "COUNTRIES_GDF = gpd.read_file('./Material/ne_10m_admin_0_countries.shp')\n",
    "\n",
    "def get_country(lon:float, lat:float) -> str:\n",
    "    if lon is None or lat is None:\n",
    "        return \"\"\n",
    "    \n",
    "    #convert to aheply point\n",
    "    point = ShapelyPoint(lon, lat)\n",
    "    country = COUNTRIES_GDF['NAME'][COUNTRIES_GDF['geometry'].contains(point)]\n",
    "    # Need to handle cases when no country found\n",
    "    country = country.item() if len(country) > 0 else \"\"\n",
    "\n",
    "    return country\n",
    "\n",
    "def to_json_cols(cols_list):\n",
    "    out_dict = {}\n",
    "    for col in cols_list:\n",
    "        F.to_json(col)\n",
    "\n",
    "udf_get_country = udf(get_country, T.StringType())\n",
    "\n",
    "\n",
    "## TBD ##\n",
    "def enrich_add_country_column(df, epoch_id):\n",
    "    logger.debug(f'in enrich_add_country_column epoch_id:{epoch_id}, with len(df): {df.count()}')\n",
    "    if df is None or df.count() <= 0:\n",
    "        return\n",
    "    minidf = df.limit(100)\n",
    "    start_enrich = time.time()\n",
    "#     df = df.withColumn('country', udf_get_country('longitude', 'latitude'))\n",
    "    df = minidf.withColumn('country', udf_get_country('longitude', 'latitude'))\n",
    "    end_enrich = time.time()\n",
    "    logger.debug(f'enriched {df.count()} records, within {end_enrich - start_enrich} seconds')\n",
    "    test_df = df.toJSON().toDF('value')\n",
    "    logger.debug(f'this is the df: {test_df.show()}')\n",
    "#     rdd = df.rdd.map(lambda row: row.asDict())\n",
    "#     logger.debug(f'rdd is {rdd.collect()}')\n",
    "    # send to kafka\n",
    "#     df.select(F.toJSON())\\\n",
    "#         .write\\\n",
    "#         .format(\"kafka\") \\\n",
    "#         .option(\"kafka.bootstrap.servers\", broker) \\\n",
    "#         .option(\"topic\", topic_enriched_real_time_states) \\\n",
    "#         .save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-16 01:11:55,220 - opensky.spark_consumer - INFO - write_to_hive: epoch_id: 0 len: 0\n",
      "2020-01-16 01:12:38,634 - opensky.spark_consumer - INFO - write_to_hive: epoch_id: 1 len: 12718\n",
      "2020-01-16 01:12:54,842 - opensky.spark_consumer - INFO - write_to_hive: epoch_id: 2 len: 12710\n",
      "2020-01-16 01:13:08,659 - opensky.spark_consumer - INFO - write_to_hive: epoch_id: 3 len: 12730\n",
      "2020-01-16 01:13:23,377 - opensky.spark_consumer - INFO - write_to_hive: epoch_id: 4 len: 12728\n",
      "2020-01-16 01:13:40,646 - opensky.spark_consumer - INFO - write_to_hive: epoch_id: 5 len: 12748\n",
      "2020-01-16 01:13:56,630 - opensky.spark_consumer - INFO - write_to_hive: epoch_id: 6 len: 12729\n",
      "2020-01-16 01:14:17,567 - opensky.spark_consumer - INFO - write_to_hive: epoch_id: 7 len: 12737\n",
      "2020-01-16 01:14:35,260 - opensky.spark_consumer - INFO - write_to_hive: epoch_id: 8 len: 12732\n",
      "2020-01-16 01:14:56,374 - opensky.spark_consumer - INFO - write_to_hive: epoch_id: 9 len: 12725\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", broker) \\\n",
    "    .option(\"subscribe\", topic_real_time_states) \\\n",
    "    .option(\"startingOffsets\", \"latest\")\\\n",
    "    .load()\n",
    "\n",
    "state_vectors_df = df.select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias(\"value\"))\\\n",
    "                        .select((F.explode(\"value\").alias(\"value\")))\\\n",
    "                        .select(\"value.*\")\n",
    "\n",
    "# # Write to parquet file\n",
    "# TBD - handle target path creation\n",
    "parquet_path = 'hdfs://cnt7-naya-cdh6.org:8020/FinalProject/parquet_archive'\n",
    "parquet_checkpoint_path = \"/home/naya/parquet_checkpoint\"\n",
    "parquet_write = state_vectors_df\\\n",
    "                .withColumn('date_hour', F.date_trunc('hour', state_vectors_df.time))\\\n",
    "                .writeStream\\\n",
    "                .outputMode(\"append\")\\\n",
    "                .format(\"parquet\")\\\n",
    "                .partitionBy('date_hour')\\\n",
    "                .option(\"checkpointLocation\", parquet_checkpoint_path)\\\n",
    "                .option(\"path\", parquet_path)\\\n",
    "                .start()\n",
    "\n",
    "tables_write = state_vectors_df\\\n",
    "                .writeStream\\\n",
    "                .foreachBatch(write_to_tables)\\\n",
    "                .start()\n",
    "\n",
    "# # TBD # #\n",
    "# enrich_with_countries = state_vectors_df\\\n",
    "#             .writeStream\\\n",
    "#             .foreachBatch(enrich_add_country_column)\\\n",
    "#             .start()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
