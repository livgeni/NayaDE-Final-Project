{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from sys import stdout\n",
    "\n",
    "formatter = logging.Formatter(\n",
    "    '%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "console_handler = logging.StreamHandler(stdout)\n",
    "console_handler.setFormatter(formatter)\n",
    "\n",
    "logger = logging.getLogger('opensky.producer')\n",
    "logger.addHandler(console_handler)\n",
    "logger.setLevel('INFO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensky_api import OpenSkyApi\n",
    "from kafka import KafkaProducer\n",
    "import json\n",
    "from time import sleep\n",
    "import pyarrow as pa\n",
    "import os\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "\n",
    "# # Topics/Brokers\n",
    "topic_real_time_states = 'real-time-states'\n",
    "topic_sparse_states = 'sparse_states'\n",
    "brokers = ['localhost:9092']\n",
    "\n",
    "\n",
    "size_mb = lambda x : x/1024/1024\n",
    "\n",
    "API = OpenSkyApi('livgeni', '1abc23')\n",
    "\n",
    "from requests.exceptions import ReadTimeout\n",
    "\n",
    "def open_sky_generator(rate_s:int):\n",
    "    while True:\n",
    "        try:\n",
    "            states = API.get_states()\n",
    "        except ReadTimeout as rte:\n",
    "            logger.warning(rte)\n",
    "        except Exception as e:\n",
    "            logger.error(e)\n",
    "        else:\n",
    "            yield states\n",
    "        sleep(rate_s)\n",
    "\n",
    "\n",
    "RED_FIELDS = ['origin_country', 'sensors']\n",
    "YELLOW_FIELDS = ['time_position', 'heading', 'vertical_rate', 'spi']\n",
    "IGNORE_FIELDS = RED_FIELDS + YELLOW_FIELDS\n",
    "\n",
    "\n",
    "def opensky_to_dict(opensky_states):\n",
    "    \"\"\"returns a list of dictionaries\"\"\"\n",
    "    svdl = list()\n",
    "    \n",
    "    try:\n",
    "        for sv in opensky_states.states:\n",
    "            svd = dict(time = opensky_states.time)\n",
    "            for key in sv.keys:\n",
    "                if key not in IGNORE_FIELDS:\n",
    "                    val = sv.__dict__[key]\n",
    "                    # Adapt to work with Spark Stream that has 0/1 for bool\n",
    "                    if type(val) == bool:\n",
    "                        svd[key] = 0 if val == False else 1\n",
    "                    else:\n",
    "                        svd[key] = val if type(val) != str else val.strip()\n",
    "            svdl += [svd]\n",
    "    except Exception as e:\n",
    "        log.error(e)\n",
    "    return svdl\n",
    "#     state_vector_json_list = json.dumps(svdl)\n",
    "#     return state_vector_json_list\n",
    "\n",
    "\n",
    "        \n",
    "class OpenskyArchiver:\n",
    "    \n",
    "    def __init__(self):\n",
    "        # hdfs config\n",
    "        self.hdfs_config = dict(\n",
    "                            hdfs_host='localhost',\n",
    "                            hdfs_port=8020,\n",
    "                            hdfs_user='hdfs',\n",
    "                            hdfs_driver='libhdfs',\n",
    "                            hdfs_archive_path = '/FinalProject/Archive'\n",
    "                               )\n",
    "        # local file configs\n",
    "        self.local_files_folder = os.path.abspath('/home/naya/tutorial/open-sky/tmp_storage')\n",
    "        self.local_files_prefix = 'opensky_state_vectors_json'\n",
    "        self.local_file_size_threshold = 100 #MB\n",
    "        self.gen_time_suffix = lambda : datetime.now().strftime(\"%Y-%m-%d_%H_%M_%S\")\n",
    "        self.gen_local_file_path = lambda : os.path.join\\\n",
    "                                    (self.local_files_folder,\\\n",
    "                                     f'{self.local_files_prefix}_{self.gen_time_suffix()}.json')\n",
    "        self.local_file_path = self.gen_local_file_path()\n",
    "        \n",
    "        # init local and hdfs folders\n",
    "        fs = pa.hdfs.connect(\n",
    "            host=self.hdfs_config['hdfs_host'],\n",
    "            port=self.hdfs_config['hdfs_port'], \n",
    "            user=self.hdfs_config['hdfs_user'], \n",
    "            kerb_ticket=None, \n",
    "            driver=self.hdfs_config['hdfs_driver'], \n",
    "            extra_conf=None)\n",
    "\n",
    "        # Create local folder if not exists\n",
    "        if os.path.exists(self.local_files_folder):\n",
    "            shutil.rmtree(self.local_files_folder)\n",
    "        os.makedirs(self.local_files_folder)\n",
    "\n",
    "        # create hdfs folder if not exist\n",
    "        hdfs_archive_path = self.hdfs_config['hdfs_archive_path']\n",
    "        if not fs.exists(hdfs_archive_path):\n",
    "            fs.mkdir(hdfs_archive_path, create_parents=True)\n",
    "        \n",
    "        self.fs = fs\n",
    "        \n",
    "    def archive_data(self, jsn_str):\n",
    "        \"\"\"\n",
    "        load as json current file if exists, and append the new json string and dump back to the file\n",
    "        if file does not exist create new and dump the new file    \n",
    "        \"\"\" \n",
    "        if os.path.isfile(self.local_file_path) and os.path.getsize(self.local_file_path) > 0:\n",
    "            with open(self.local_file_path, 'r') as local_file:\n",
    "                file_jsn = json.load(local_file)\n",
    "        else:\n",
    "            file_jsn = json.loads('[]')\n",
    "        with open(self.local_file_path, 'w') as local_file:\n",
    "            new_jsn = json.loads(jsn_str)\n",
    "            file_jsn.extend(new_jsn)\n",
    "            json.dump(file_jsn, local_file)\n",
    "#             local_file.write(jsn_str)\n",
    "        # test if target file size reached\n",
    "        if size_mb(os.path.getsize(self.local_file_path)) >= self.local_file_size_threshold:\n",
    "            # upload to hdfs\n",
    "            logger.debug(f'Uploading to HDFS')\n",
    "            with open(self.local_file_path, 'rb') as source:\n",
    "                target_file_name = os.path.split(self.local_file_path)[1]\n",
    "                target_full_path = f'{self.hdfs_config[\"hdfs_archive_path\"]}/{target_file_name}'\n",
    "                self.fs.upload(target_full_path, source)\n",
    "            logger.debug(f'Uploaded to : {target_full_path}')\n",
    "            # delete the local file\n",
    "            logger.debug(f'deleting local file {self.local_file_path}')\n",
    "            os.remove(self.local_file_path)\n",
    "            # new name for new file\n",
    "            self.local_file_path = self.gen_local_file_path()\n",
    "            logger.debug(f'generated new local_file_name :{os.path.split(self.local_file_path)[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to Archiving Folder in HDFS\n",
    "\n",
    "1. Append to a local file until reaching 100MB\n",
    "2. When reached - upload to HDFS using pyarrow and open a new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "producer = KafkaProducer(bootstrap_servers=brokers, max_request_size = 4299162)\n",
    "\n",
    "archiver = OpenskyArchiver()\n",
    "rt_rate_s = 10\n",
    "sparse_counter = 0\n",
    "alerts_rate_s = 3 * 60\n",
    "\n",
    "\n",
    "for states in open_sky_generator(rt_rate_s):\n",
    "    if states is None:\n",
    "        logger.warning('Received empty states vector')\n",
    "        continue\n",
    "    \n",
    "    message_json = json.dumps(opensky_to_dict(states))\n",
    "    # First archive the message\n",
    "    archiver.archive_data(message_json)\n",
    "    # Then send to kafka\n",
    "    try:\n",
    "        send_result = producer.send(topic_real_time_states, value = message_json.encode('utf-8'))\n",
    "        if send_result.exception:\n",
    "            logger.error(f'producer send ecxeption: {send_result.exception}')\n",
    "            logger.debug(f\"sent time : {datetime.fromtimestamp(states.time)} ; len of sent message : {len(message_json)}\")\n",
    "    \n",
    "        if (sparse_counter * rt_rate_s) >= alerts_rate_s:\n",
    "            sparse_counter = 0\n",
    "            send_result = producer.send(topic_sparse_states, value = message_json.encode('utf-8'))\n",
    "            if send_result.exception:\n",
    "                logger.error(f'producer send ecxeption: {send_result.exception}')\n",
    "                logger.debug(f\"sent time : {datetime.fromtimestamp(states.time)} ; len of sent message : {len(message_json)}\")\n",
    "        else:\n",
    "            sparse_counter += 1\n",
    "    except Exception as ex:\n",
    "        logger.error(ex)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
